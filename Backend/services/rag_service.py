# RAGæœåŠ¡å±‚ï¼Œå¤„ç†æ£€ç´¢å¢å¼ºç”Ÿæˆç›¸å…³çš„ä¸šåŠ¡é€»è¾‘
from typing import List, TypedDict, Generator
from langchain.schema import Document
from langchain.prompts import PromptTemplate
from langchain_deepseek import ChatDeepSeek
from langgraph.graph import START, StateGraph
from bson import ObjectId

from services.vector_service import VectorService
from config.settings import ai_config
from config.database import db_client

class RAGService:
    """RAGæ£€ç´¢å¢å¼ºç”ŸæˆæœåŠ¡"""
    
    def __init__(self):
        """åˆå§‹åŒ–RAGæœåŠ¡"""
        self.vector_service = VectorService()
        
        # åˆå§‹åŒ–DeepSeek LLM
        self.llm = ChatDeepSeek(
            model='deepseek-chat',
            temperature=0.3,
            streaming=True,
            api_key=ai_config.deepseek_api_key
        )
        
        # å®šä¹‰QAæç¤ºæ¨¡æ¿
        self.qa_prompt = PromptTemplate(
            input_variables=["context", "question"],
            template="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸š AI åŠ©æ‰‹ã€‚ä»¥ä¸‹æ˜¯ä½ å¯ä»¥å‚è€ƒçš„æ–‡æ¡£ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·ç»“åˆæ–‡æ¡£å†…å®¹å›ç­”ï¼Œå¹¶åœ¨å¿…è¦æ—¶ç»™å‡ºå‚è€ƒæ¥æº,å¹¶é™„ä¸Šå‚è€ƒæ¥æºçš„æ–‡æ¡£IDã€‚

ğŸ”¥ é‡è¦ï¼šè¯·ä½¿ç”¨ HTML æ ¼å¼è¾“å‡ºï¼Œç¡®ä¿æ ¼å¼æ¸…æ™°ç¾è§‚ã€‚

æ ¼å¼è¦æ±‚ï¼š
1. ä½¿ç”¨ <p> æ ‡ç­¾åŒ…è£¹æ®µè½
2. ä½¿ç”¨ <strong> æˆ– <b> æ ‡ç­¾æ ‡è¯†é‡è¦å†…å®¹
3. ä½¿ç”¨ <em> æˆ– <i> æ ‡ç­¾æ ‡è¯†å¼ºè°ƒå†…å®¹
4. ä½¿ç”¨ <ul> å’Œ <li> æ ‡ç­¾åˆ›å»ºæ— åºåˆ—è¡¨
5. ä½¿ç”¨ <ol> å’Œ <li> æ ‡ç­¾åˆ›å»ºæœ‰åºåˆ—è¡¨
6. ä½¿ç”¨ <br> æ ‡ç­¾æ¢è¡Œ
7. ä½¿ç”¨ <code> æ ‡ç­¾æ ‡è¯†ä»£ç æˆ–ç‰¹æ®Šæ–‡æœ¬
8. ä½¿ç”¨ <blockquote> æ ‡ç­¾æ ‡è¯†å¼•ç”¨
9. ä½¿ç”¨ <table>ã€<thead>ã€<tbody>ã€<tr>ã€<th>ã€<td> æ ‡ç­¾åˆ›å»ºè¡¨æ ¼
10. ä¸è¦ä½¿ç”¨ Markdown è¯­æ³•ï¼Œåªä½¿ç”¨çº¯ HTML

ç¤ºä¾‹è¾“å‡ºæ ¼å¼ï¼š
<p>æ ¹æ®æ–‡æ¡£å†…å®¹ï¼Œä»¥ä¸‹æ˜¯ç­”æ¡ˆï¼š</p>
<p><strong>ä¸»è¦è§‚ç‚¹ï¼š</strong>è¿™æ˜¯é‡è¦å†…å®¹ã€‚</p>
<ul>
  <li>ç¬¬ä¸€ç‚¹è¯´æ˜</li>
  <li>ç¬¬äºŒç‚¹è¯´æ˜</li>
</ul>
<p>è¯¦ç»†è¯´æ˜æ–‡æœ¬...</p>

è¯·ä¸¥æ ¼æŒ‰ç…§ HTML æ ¼å¼è¾“å‡ºï¼Œä¸è¦ä½¿ç”¨ Markdownã€‚

"""


# æ ¼å¼è¦æ±‚ï¼š
# 1. ä½¿ç”¨æ¢è¡Œç¬¦åˆ†éš”æ®µè½ï¼Œç¡®ä¿è¾“å‡ºå†…å®¹ç»“æ„æ¸…æ™°
# 2. æ•°å­¦å…¬å¼ä½¿ç”¨ LaTeX æ ¼å¼ï¼Œè¡Œå†…å…¬å¼ä½¿ç”¨ $...$ æ ‡è¯†
# 3. å—çº§å…¬å¼ä½¿ç”¨ $$...$$
# 4. é‡è¦æ¦‚å¿µæˆ–å…³é”®è¯ä½¿ç”¨ **ç²—ä½“** æ ‡è¯†
# 5. å¦‚æœ‰åˆ—è¡¨ï¼Œè¯·ä½¿ç”¨æ ‡å‡†çš„ Markdown æ ¼å¼

# ç¤ºä¾‹ï¼š
# å½“è®¨è®ºç‰©ç†å®šå¾‹æ—¶ï¼Œç‰›é¡¿ç¬¬äºŒå®šå¾‹å¯è¡¨ç¤ºä¸ºï¼š
# $$F = ma$$
# å…¶ä¸­ $F$ æ˜¯åŠ›ï¼Œ$m$ æ˜¯è´¨é‡ï¼Œ$a$ æ˜¯åŠ é€Ÿåº¦ã€‚

        )
        
        # æ„å»ºRAGé“¾
        self.rag_chain = self._build_rag_chain()
    
    def _build_rag_chain(self):
        """æ„å»ºRAGå¤„ç†é“¾"""
        
        # å®šä¹‰çŠ¶æ€ç±»å‹
        class State(TypedDict):
            question: str
            user_id: str 
            continue_chat: bool
            context: List[Document]
            answer: str
        
        def retrieve(state: State):
            """æ£€ç´¢æ­¥éª¤ï¼šæ ¹æ®é—®é¢˜æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
            query = state['question']
            user_id = state.get('user_id')
            
            # è·å–å‘é‡æœç´¢ç»“æœ
            results = self.vector_service.search_embedding(query, user_id, top_k=5)
            
            docs = []
            for score, doc_id in results:
                # ä»todosContenté›†åˆè·å–å®Œæ•´å†…å®¹
                content = db_client.todosContent.find_one({"_id": ObjectId(doc_id)})
                if content and content['user_id'] == user_id:
                    # åˆå¹¶ç”¨æˆ·å†…å®¹å’Œæå–å†…å®¹
                    full_content_parts = []
                    
                    # ç”¨æˆ·è¾“å…¥çš„å†…å®¹
                    if content.get("content"):
                        full_content_parts.append(content["content"])
                    
                    # æå–çš„å†…å®¹
                    if content.get("extracted_content"):
                        extracted = content["extracted_content"]
                        
                        # OCRæ–‡æœ¬
                        if extracted.get("ocr_texts") and len(extracted["ocr_texts"]) > 0:
                            ocr_section = "ã€å›¾ç‰‡è¯†åˆ«å†…å®¹ã€‘\n" + "\n".join(extracted["ocr_texts"])
                            full_content_parts.append(ocr_section)
                        
                        # æ–‡æ¡£æ–‡æœ¬
                        if extracted.get("file_texts") and len(extracted["file_texts"]) > 0:
                            file_section = "ã€æ–‡æ¡£æå–å†…å®¹ã€‘\n" + "\n".join(extracted["file_texts"])
                            full_content_parts.append(file_section)
                    
                    # åˆå¹¶æ‰€æœ‰å†…å®¹ä¼ ç»™LLM
                    full_content = "\n\n".join(full_content_parts)
                    
                    docs.append(Document(
                        page_content=full_content,
                        metadata={
                            "doc_id": doc_id,
                            "score": score,
                            "user_id": user_id
                        }
                    ))
            
            return {"context": docs}
        
        def generate(state: State):
            """ç”Ÿæˆæ­¥éª¤ï¼šåŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£ç”Ÿæˆå›ç­”"""
            docs_content = "\n\n".join(doc.page_content for doc in state["context"])
            
            # å¦‚æœæ˜¯ç»§ç»­å¯¹è¯ï¼Œæ·»åŠ æç¤º
            continue_prefix = ""
            if state.get("continue_chat", False):
                continue_prefix = "è¿™æ˜¯ç”¨æˆ·é‡æ–°è¿›å…¥å¯¹è¯çš„ç»§ç»­ã€‚è¯·ç»§ç»­ä¹‹å‰çš„å›ç­”ã€‚\n"
            
            # æ ¼å¼åŒ–æç¤º
            formatted_prompt = self.qa_prompt.format(
                context=docs_content,
                question=f"{continue_prefix}{state['question']}"
            )
            
            # è¿”å›ç”Ÿæˆå™¨ï¼Œæ–¹ä¾¿SSEæµå¼è¾“å‡º
            def stream_answer():
                try:
                    for chunk in self.llm.stream(formatted_prompt):
                        yield chunk.content
                    
                    # ç”Ÿæˆç»“æŸåï¼Œé™„åŠ å‚è€ƒæ¥æº
                    if state["context"]:
                        yield "\n\nå‚è€ƒæ–‡æ¡£ï¼š\n" + "\n".join(
                            [f"- doc_id: {doc.metadata['doc_id']} (score={doc.metadata['score']:.4f})"
                             for doc in state["context"]]
                        )
                except Exception as e:
                    yield f"\n\nç”Ÿæˆå›ç­”æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
            
            return {"answer": stream_answer}
        
        # æ„å»ºçŠ¶æ€å›¾
        graph_builder = StateGraph(State)

        graph_builder.add_node("retrieve", retrieve)
        graph_builder.add_node("generate", generate)
        graph_builder.add_edge(START, "retrieve")
        graph_builder.add_edge("retrieve", "generate")
        
        return graph_builder.compile()
    
    def process_question(self, question: str, user_id: str, continue_chat: bool = False) -> dict:
        """
        å¤„ç†ç”¨æˆ·é—®é¢˜ï¼Œè¿”å›RAGç»“æœ
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            user_id: ç”¨æˆ·ID
            continue_chat: æ˜¯å¦ç»§ç»­å¯¹è¯
            
        Returns:
            dict: åŒ…å«answerç”Ÿæˆå™¨çš„ç»“æœ
        """
        state = {
            "question": question,
            "user_id": user_id,
            "continue_chat": continue_chat
        }
        
        return self.rag_chain.invoke(state)
    
    def get_relevant_documents(self, query: str, user_id: str, top_k: int = 5) -> List[Document]:
        """
        è·å–ç›¸å…³æ–‡æ¡£ï¼ˆä¸ç”Ÿæˆå›ç­”ï¼‰
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            user_id: ç”¨æˆ·ID
            top_k: è¿”å›æ–‡æ¡£æ•°é‡
            
        Returns:
            List[Document]: ç›¸å…³æ–‡æ¡£åˆ—è¡¨
        """
        results = self.vector_service.search_embedding(query, user_id, top_k)
        
        docs = []
        for score, doc_id in results:
            content = db_client.todosContent.find_one({"_id": ObjectId(doc_id)})
            if content and content['user_id'] == user_id:
                docs.append(Document(
                    page_content=content["content"],
                    metadata={
                        "doc_id": doc_id,
                        "score": score,
                        "user_id": user_id,
                        "todo_id": content.get("todo_id", ""),
                        "created_at": content.get("created_at", "")
                    }
                ))
        
        return docs


